---
title: "ITEC 621 Exercise 3 - Basic Models"
author: "Johnson Odejide"
date: "February 16, 2023"
output:
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
  word_document:
    toc: yes
    toc_depth: 2
subtitle: WLS, Logistic and Trees
---

```{r global_options}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

## General Instructions

Download the **Ex3_BasicModels_YourLastName.Rmd** R Markdown file and save it with your own **last name** and **date**. Complete all your work in that template file.

**Knitting: Knit** your .Rmd file into a Word, HTML or PDF file. Your knitted document **must display your R commands**. Knitting and formatting is worth up to **3 points** in this and all exercises.  

**Formatting:** Please ensure that all your text narratives are fully visible (if I can't see the text, I can't grade it). Also, please ensure that your **Table of Contents** is visible and properly formatted. Also, please prepare your R Markdown file with a **professional appearance**, as you would for top management or an important client. Please, write all your interpretation narratives in the text area, outside of the R code chunks, with the appropriate formatting and businesslike appearance. **Note:** I write all my interpretation solutions inside of the R code chunk to suppress their display until I print the solution, but don't need to do this. I will read your submission as a report to a client or senior management. Anything unacceptable to that audience is unacceptable to me.

**Important Formatting Tip About the # Tag:** Many students submit their knitted file with text narratives embedded in the table of contents and with the text in the main body in large blue font. This is **NOT** proper business formatting. This is the issue: if you want to write comments inside an R code chunk, you need to use the # tag, which tells R that that line should not be executed and it is there as a comment only. However, if you use the # tag in the text area, R Markdown treats this as **Heading 1** text and ## as **Heading 2** text. Heading text will appear in the table of contents and in large blue font in the main text. Please **DO NOT** use # tags in the main text, except for actual headers and sub-headers in your document.

**Submission**: Submit your knitted homework document in Canvas. There is no need to submit the .Rmd file, just your knitted file.  

## Setup

This analysis will be done with the **Hitters{ISLR}** baseball player data set, using AtBat, Hits, Walks, PutOuts, Assists and HmRun as predictors and player **Salary** as the outcome variable. Also, set the `options(scipen = 4)` to minimize the use of scientific notation.

```{r}
# Prep work done for you

library(ISLR) # Contains the Hitters data set
options(scipen = 4)
```

Familiarize yourself with the Hitters data set by entering the commands below in the R Console window, but NOT in the R Markdown file. Inspect the data and the description of each predictor, to familiarize yourself with the data

`?Hitters`

`View(Hitters)`

 Let's start with an OLS model, which you will then test for heteroskedasticity.

```{r}
# Prep work done for you

# The Hitters data set has several records with missing data, let's remove them

Hitters <- na.omit(Hitters) 

# We now fit an OLS model to start with

fit.ols <- lm(Salary ~ AtBat + Hits + Walks + 
                       PutOuts + Assists + HmRun, 
                       data = Hitters)

summary(fit.ols) # Check it out

# As the output shows, there are 4 significant predictors: AtBat, Hits, Walks and PutOuts, and 2 non-significant predictors: Assists and HmRun.
```

## 1. Heteroskedasticity Testing

1.1 Inspect the residuals visually for heteroskedasticity. To do this, display the first residual `plot()` for **fit.ols** using the parameter `which = 1`.
   
```{r}
plot(fit.ols, which = 1)
```

1.2 Then load the **{lmtest}** library and conduct a **Breusch-Pagan** test for Heteroskedasticity for the **fit.ols** model above, using the `bptest()` function.

```{r}
library(lmtest)

bptest(fit.ols)
```

1.3 Is there a problem with Heteroskedasticity? Why or why not? In your answer, please refer to **both**, the residual plot and the BP test.  
    
_The residual plot shows some pattern of uneven variance which appears to have been increasing from left to right indicating the presence of Heteroskedasticity. As the predicted salaries increase (get larger), the errors also tend to increase (grow larger) as well._ 

_This is also confirmed in the result of the Breusch-Pagan test which is significant at a p-value of 0.017 showing that there is the presence of Heteroskedasticity. This condition of heteroskedasticity makes WLS a more efficient estimator than the OLS in this case_


## 2. Weighted Least Squares (WLS) Model

2.1 Let's set up the parameters of the WLS model. Let's start by using the `fitted()` function to extract the fitted (i.e., predicted) values from the **fit.ols** object created above and store the results in a vector object named **fitted.ols**. 

```{r}
fitted.ols <- fitted(fit.ols)
```

2.2 Then, use the `abs()` and `residuals()` functions, compute the absolute value of the residuals from the OLS model **fit.ols** and store the results in a vector object named **abs.res**. Then use the `cbind()` function to list the **fitted.ols** and **abs.res** values side by side for the first 10 records (tip: add the index `[1:10, ]` after the function to list only the first 10 rows and all columns)

```{r}
abs.res <- abs(residuals(fit.ols))

cbind(fitted.ols, abs.res)[1:10, ]
```

2.3 Now that you have two vectors, one with the absolute value of the residuals and one with the predicted values of the outcome variable Salary, fit an `lm()` model using **fitted.ols** as a predictor vector for the absolute value of the residuals in **abs.res** as the outcome. To check your results, display the first 10 rows of the `fitted()` values of **lm.abs.res** (tip: again, use the `[1:10]` index after the function)

**Technical tip:** Because you are using one data vector to predict another data vector, you don't need the `data =` parameter. You only need the `data =` parameter when your variables are columns in a data frame.

```{r}
lm.abs.res <- lm(abs.res ~ fitted.ols)
fitted(lm.abs.res)[1:10]
```

Think, but no need to answer. What is the difference between **fitted.ols**, **abs.res** and **fitted(lm.abs.res)**?

```{r}
# fitted.ols is a vector containing the predicted values of the OLS model

# abs.res is a vector containing the absolute values of the residuals from fitted.ols

# fitted(lm.abs.res) is a vector containing the predicted values of the absolute values of the errors. We could use abs.res for the weight vector in WLS, and some methods do that, but the problem is that the actual values of the residuals vary all over the place. We use the fitted values instead to use a smoother set of weights (from the straight line of the regression)
```

2.4 To visualize the lm.abs.res regression line, plot the **fitted.ols** vector against the **abs.res** vector. Then draw a red line using the `abline()` function for the **lm.abs.res** regression object.

**Technical Note:** Notice that I use the `fig.width` and `fig.height` attributes in the `{r` code chunk header to define the size of the plots in inches.

```{r fig.width = 8, fig.height = 6}
plot(fitted.ols, abs.res)
abline(lm.abs.res, col = "red")
```

2.5 Specify and run the **WLS** regression model. First, create a weight vector named **wts** equal to the inverse squared predicted values of **lm.abs.res** (tip: use `wts <- 1 / fitted(lm.abs.res) ^ 2`). To check things, display the first 10 rows of the **wts** vector.

```{r}
wts <- 1 / fitted(lm.abs.res) ^ 2

wts[1:10]
```

Then fit the WLS regression model using the same predictors you used in **ols.fit**, but using **wts** for the `weights =` parameter. Name this regression object **wls.fit**. Display the summary results.

While we are at it, also fit a similar weighted GLM model (**WGLM**), by using the `glm()` function and the exact same specification you used in the `lm()` function, and store the results in an object named **fit.wglm**. Then display the `summary()` results for the WGLM. 

```{r}
wls.fit <- lm(Salary ~ AtBat + Hits + Walks + 
                       PutOuts + Assists + HmRun, 
                       data = Hitters,
              weights = wts)

summary(wls.fit)

fit.wglm <- glm(Salary ~ AtBat + Hits + Walks + 
                       PutOuts + Assists + HmRun,
                data = Hitters,
                weights = wts,
                )
summary(fit.wglm)
```

2.6 Observe the similarities an differences between the OLS, WLS and WGLM model and provide a brief commentary of your observations.

* The WLS and WGLM yielded the same result except that the statistics are reported differently. For instance the WGLM reported the deviance while WLS reported the p-value, F-Statistics, R-Squared, etc.

* At p-value<0.05, **Assists** and **HmRun** are not significant in the OLS model, they are however significant in WLS and WGLM models. The remaining four (4) significant predictors in the OLS model remained significant in the WLS and WGLM models. 

* Another thing to note is that the value of R-Squared got reduced in the WLS model from 0.311 to 0.175. However, we cannot compare these two R-Squared as that of WLS had to do with the weighted variance while WGLM has to do with deviance (2LL).


## 3. Logistic Regression

3.1 Download the **myopia.csv** file to your working directory. Then read it using `read.table()` with the parameters `header = T, row.names = 1, sep = ","`. Store the data set in an object named **myopia**. 

Please review the data set documentation at: https://rdrr.io/cran/aplore3/man/myopia.html

please note that **myopic** is coded as 1 (Yes), 0 (No), not as 1 and 2.

For sanity check, list the first 10 rows and 8 columns of this data set.

```{r}
myopia <- read.table("../../Dataset/myopia.csv", header = T, row.names = 1, sep = ",")

myopia[1:10, 1:8]
```

3.2 Fit a logistic model to predict whether a child is **myopic**, using `age + female + sports.hrs + read.hrs + mommy + dadmy` as predictors. Use the parameters `family = "binomial"(link = "logit")` to specify the Logistic model. Store the results in an object named **myopia.logit**. Display the `summary()` results. Then display the `summary()` results.

```{r}
myopia.logit <- glm(myopic ~ age + female + sports.hrs + read.hrs + mommy + dadmy, family = "binomial"(link = "logit"), data = myopia)

summary(myopia.logit)
```

3.3 For interpretation purposes, display the log-odds alongside the odds. Use the `coef()` function to extract the log-odds coefficients from **myopia.logit** and save them in a vector object named **log.odds**. Then use the `exp()` function to convert the log-odds into odds and store the results in a vector object named **odds**. 

```{r}
log.odds <- coef(myopia.logit)

odds <- exp(log.odds)
```

3.4 Finally, list the log-odds and odds side by side using the `cbind()` function. Name the columns as shown in the display below. Once you test that your `cbind()` function is working correctly, embed the function inside the `print()` function with the parameter `digits = 2` to get a more compact display.

```{r}
print(cbind("Log-Odds" = log.odds, "Odds" = odds), digits = 2)
```
 
3.5 Provide a brief interpretation of both, the log-odds and odds effects of **read.hrs** and **mommy**. Please refer to the respective variable **measurement units** in your discussion.

_Both effects are significant._ 

_For **read.hrs**, we estimate that on average, holding everything else constant, for each additional hours per week outside of school the child spent reading for pleasure, the log odds of being myopic within the first five years of follow up increases by 0.799 while the Odds increase by a factor of 2.22._

_Similarly, for **mommy**, one could say that on average, holding everything else constant, if the mother of the child is myopic, the log-odds of the child being myopic within the first five years of follow up increases by 2.937 while the Odds increase by a factor of 18.87_


## 4. Decision Trees

**4.1 Regression Tree**. Load the **{tree}** library. Then fit a regression tree with the same specification as the regression model **ols.fit** above. Use the `tree()` function and save the results in an object named **fit.tree.salary**. Then plot the tree using the `plot()` and `text()` functions (use the `pretty = 0` parameter). Also use the `title()` function to title you tree diagram **Baseball Salaries Regression Tree**.

```{r fig.width = 10, fig.height = 8}
library(tree)

fit.tree.salary <- tree(Salary ~ AtBat + Hits + Walks + 
                       PutOuts + Assists + HmRun, 
                       data = Hitters)

plot(fit.tree.salary)
text(fit.tree.salary, pretty = 0)
title("Baseball Salaries Regression Tree")
```

4.2 Classification Tree. 

Before you start, check the `class()` of the `myopia$myopic` variable and you will notice that it is an integer, not a factor (categorical) variable. This works fine in a Logistic regression model, but a factor outcome variable gives you better visual displays in classification trees. Let's create the corresponding factor variable with `myopia$myopic.f <- as.factor(myopia$myopic)`. Notice that we are renaming the outcome variable so that we don't disturb the original variables. To be certain that the vector was converted from text to factor, list the `class()` of the `myopia$myopic.f` vector.

```{r}
class(myopia$myopic)

myopia$myopic.f <- as.factor(myopia$myopic)

class(myopia$myopic.f)
```

Fit a classification tree model using the same specification as the Logistic model **myopia.logit**, but using `myopic.f` as the outcome variable. Use the `tree()` function and save the results in an object named **fit.tree.myopia**. Then plot the tree using the `plot()` and `text()` functions (use the `pretty = 0` parameter). Also use the `title()` function to title you tree diagram **Myopia Classification Tree**.

```{r fig.width = 10, fig.height = 8}
fit.tree.myopia <- tree(myopic.f ~ age + female + sports.hrs + read.hrs + mommy + dadmy, data = myopia)

plot(fit.tree.myopia)
text(fit.tree.myopia, pretty = 0)
title("Myopia Classification Tree")
```
